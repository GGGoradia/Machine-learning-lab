{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3aed33",
   "metadata": {},
   "source": [
    "Autoencoders implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3663ac",
   "metadata": {},
   "source": [
    "- can be used for Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22117902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "235/235 [==============================] - 3s 5ms/step - loss: 0.6930 - val_loss: 0.6929\n",
      "Epoch 2/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6927 - val_loss: 0.6926\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6926 - val_loss: 0.6924\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6924 - val_loss: 0.6923\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6922 - val_loss: 0.6921\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6921 - val_loss: 0.6919\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6919 - val_loss: 0.6918\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6917 - val_loss: 0.6916\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6916 - val_loss: 0.6914\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6914 - val_loss: 0.6913\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6912 - val_loss: 0.6911\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6911 - val_loss: 0.6909\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6909 - val_loss: 0.6907\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6907 - val_loss: 0.6906\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6906 - val_loss: 0.6904\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6904 - val_loss: 0.6902\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6902 - val_loss: 0.6900\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6900 - val_loss: 0.6899\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6899 - val_loss: 0.6897\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6897 - val_loss: 0.6895\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6895 - val_loss: 0.6893\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6893 - val_loss: 0.6891\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6891 - val_loss: 0.6889\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6889 - val_loss: 0.6887\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6887 - val_loss: 0.6885\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6885 - val_loss: 0.6883\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6883 - val_loss: 0.6881\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6881 - val_loss: 0.6879\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6879 - val_loss: 0.6877\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6877 - val_loss: 0.6875\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6875 - val_loss: 0.6873\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6873 - val_loss: 0.6871\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6871 - val_loss: 0.6868\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6868 - val_loss: 0.6866\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6866 - val_loss: 0.6863\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6864 - val_loss: 0.6861\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6861 - val_loss: 0.6859\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6859 - val_loss: 0.6856\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6856 - val_loss: 0.6853\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6854 - val_loss: 0.6851\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6851 - val_loss: 0.6848\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6848 - val_loss: 0.6845\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6845 - val_loss: 0.6842\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6842 - val_loss: 0.6839\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6840 - val_loss: 0.6836\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6836 - val_loss: 0.6833\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6833 - val_loss: 0.6830\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6830 - val_loss: 0.6827\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.6827 - val_loss: 0.6823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1219c7fab60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input,Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "encoding=32\n",
    "inputimg=Input(shape=(784,))#shape of the input\n",
    "encoded=Dense(encoding,activation='relu')(inputimg)#create layer then second bracket join the layer with a defined layer\n",
    "\n",
    "decoded=Dense(784,activation='sigmoid')(encoded)#sigmoid used as values are scaled between 0 to 1 by dividing 255\n",
    "\n",
    "autoencoder=Model(inputimg,decoded)#the whole model\n",
    "encoder=Model(inputimg,encoded)#the encoder part\n",
    "\n",
    "encodedinput=Input(shape=(encoding,))\n",
    "decodedlayer=autoencoder.layers[-1]#last layer of the autoencoder model which is (784,)\n",
    "decoder=Model(encodedinput,decodedlayer(encodedinput))#decodedlayer(encodedinput)joins the 2 layers together so essentially its the decoded part of the model\n",
    "\n",
    "#separated encoder used for feature extraction and decoder can be used for data generation\n",
    "autoencoder.compile(optimizer='adadelta',loss='binary_crossentropy')\n",
    "#training\n",
    "(xtrain,_),(xtest,_)=mnist.load_data()\n",
    "\n",
    "#scaling the value betweemn 0 and 1\n",
    "xtrain=xtrain.astype('float32')/255\n",
    "xtest=xtest.astype('float32')/255\n",
    "\n",
    "xtrain=xtrain.reshape(len(xtrain),np.prod(xtrain.shape[1:]))#data in shape(X,y,y) reshaped into (X,Y*Y)\n",
    "xtest=xtest.reshape(len(xtest),np.prod(xtest.shape[1:]))#similar to above\n",
    "\n",
    "autoencoder.fit(xtrain,xtrain,epochs=50,batch_size=256,validation_data=(xtest,xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f270671",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m=\u001b[39mModel(x,x_decoded_mea)\n\u001b[0;32m     32\u001b[0m xentloss\u001b[38;5;241m=\u001b[39mogdim\u001b[38;5;241m*\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mbinary_crossentropy(x,x_decoded_mea)\u001b[38;5;66;03m#measures how close the predicted value is\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m kl_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39mK\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mzlogvar\u001b[38;5;241m-\u001b[39m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39mK\u001b[38;5;241m.\u001b[39mexp(zlogvar),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#axis =-1 makes the summation over latent dimensions as in shape(batchsize,latentdim)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m vaeloss\u001b[38;5;241m=\u001b[39mK\u001b[38;5;241m.\u001b[39mmean(xentloss\u001b[38;5;241m+\u001b[39mkl_loss)\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39madd_loss(vaeloss)\n",
      "File \u001b[1;32mc:\\Users\\devgo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\backend.py:2080\u001b[0m, in \u001b[0;36msquare\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   2077\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.square\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare\u001b[39m(x):\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\devgo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32mc:\\Users\\devgo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:13363\u001b[0m, in \u001b[0;36msquare\u001b[1;34m(x, name)\u001b[0m\n\u001b[0;32m  13361\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m  13362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m> 13363\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msquare_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  13364\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  13365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m  13366\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\devgo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:13404\u001b[0m, in \u001b[0;36msquare_eager_fallback\u001b[1;34m(x, name, ctx)\u001b[0m\n\u001b[0;32m  13403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquare_eager_fallback\u001b[39m(x: Annotated[Any, TV_Square_T], name, ctx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Annotated[Any, TV_Square_T]:\n\u001b[1;32m> 13404\u001b[0m   _attr_T, (x,) \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplex128\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  13405\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x]\n\u001b[0;32m  13406\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n",
      "File \u001b[1;32mc:\\Users\\devgo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:251\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[1;32m--> 251\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[0;32m    253\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[0;32m    254\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
      "File \u001b[1;32mc:\\Users\\devgo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:209\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    207\u001b[0m overload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__tf_tensor__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 209\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[0;32m    212\u001b[0m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[0;32m    213\u001b[0m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\devgo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Dense,Lambda\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "ogdim=784\n",
    "latentdim=2\n",
    "interdim=256\n",
    "\n",
    "x=Input(shape=(ogdim,))\n",
    "h=Dense(interdim,activation='relu')(x)#needed a non linear function to stack on the layer and also avoid vanishing gradient\n",
    "zmean=Dense(latentdim)(h)\n",
    "zlogvar=Dense(latentdim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    zlogvar,zmean=args\n",
    "    #epsilon random noise mean=0 and std=1\n",
    "    epsilon=K.random_normal(shape=(K.shape(zmean)[0],latentdim),mean=0.,stddev=1.0)#random_normal creates random numbers and then the guassian noise is generated\n",
    "    return zmean+K.exp(zlogvar/2)*epsilon #formula for guassian noise\n",
    "\n",
    "z=Lambda(sampling,output_shape=(latentdim,))([zmean,zlogvar])#defines your own transformation and creates the layer instead of the default ones \n",
    "# here the custom functioning layer is sampling through which the layer is used\n",
    "decoderh=Dense(interdim,activation='relu')\n",
    "decodermean=Dense(ogdim,activation='sigmoid')# since output is 1 or 0 as required so sigmoid\n",
    "\n",
    "h_decoded=decoderh(z)\n",
    "x_decoded_mea=decodermean(h_decoded)\n",
    "\n",
    "model=Model(x,x_decoded_mea)\n",
    "\n",
    "xentloss=ogdim*metrics.binary_crossentropy(x,x_decoded_mea)#measures how close the predicted value is\n",
    "kl_loss=-0.5*K.sum(1+zlogvar-K.square(zmean)-K.exp(zlogvar),axis=1)#axis =-1 makes the summation over latent dimensions as in shape(batchsize,latentdim)\n",
    "\n",
    "vaeloss=K.mean(xentloss+kl_loss)\n",
    "model.add_loss(vaeloss)\n",
    "model.compile(optimizer='rmsprop')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446c2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.reshape((-1, 28, 28, 1))\n",
    "xtest = xtest.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# Normalize (optional but recommended)\n",
    "xtrain = xtrain.astype('float32') / 255.\n",
    "xtest = xtest.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408db1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 12s 4ms/step - loss: 0.0193 - val_loss: 0.0041\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0036 - val_loss: 0.0037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1220933aad0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convolutional auto encoders\n",
    "from tensorflow.keras.layers import Input,Dense,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from keras.models import Model\n",
    "\n",
    "inputimg=Input(shape=(28,28,1))\n",
    "x=Conv2D(16,(3,3),activation='relu',padding='same')(inputimg)#uses zeroes as padding for the digital image\n",
    "x=MaxPooling2D((2,2),padding='same')(x)#reduces the size of computations and makes less sensitive to shifts in image\n",
    "x=Conv2D(8,(3,3),activation='relu',padding='same')(x)\n",
    "x=MaxPooling2D((2,2),padding='same')(x)\n",
    "x=Conv2D(16,(3,3),padding='same',activation='relu')(x)\n",
    "x=UpSampling2D((2,2))(x)#since maxpooling downsamples we need to later upsample\n",
    "x=UpSampling2D((2,2))(x)\n",
    "decoded=Conv2D(1,(3,3),padding='same',activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "autoencoder=Model(inputimg,decoded)\n",
    "autoencoder.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "autoencoder.fit(xtrain,xtrain,epochs=5,shuffle=True,validation_data=(xtest,xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6644d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
